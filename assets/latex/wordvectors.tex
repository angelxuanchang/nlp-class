\input{beamerpreamble.tex}
\begin{document}
\input{titlepage.tex}

\lecture{Word Vectors}{}

\section{One-hot vectors}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{One-hot vectors}
\begin{itemize}[<+->]
\item Let $|V|$ be the size of the vocabulary
\item Assign each word to a unique index from $1 \ldots |V|$
\item e.g. {\em aarvark} is $1$, {\em a} is $2$, etc.
\item Represent each word as as a $\mathbb{R}^{|V|\times 1}$
\item The vector has one at index $i$ and all other values are $0$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{One-hot vectors}
\framesubtitle{Figure from \cite{cs224n}}
\begin{center}
\includegraphics[scale=.45]{figures/wordvectors/onehot}	
\end{center}
\end{frame}

\begin{frame}
\frametitle{One-hot vectors}
\begin{itemize}[<+->]
\item Problems with similarity over one-hot vectors 
\item Consider similarity between words as dot product between their word vectors:
\[ w_{\textrm{cat}} \cdot w_{\textrm{dog}} = 
w_{\textrm{joker}} \cdot w_{\textrm{dog}} = 
0 \]
\item Idea: reduce the size of the large sparse one-hot vector
\item Embed large sparse vector into a dense subspace.
\end{itemize}
\end{frame}

\section{Singular Value Decomposition}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Window based co-occurrence matrix}
\begin{itemize}[<+->]
\item Assume a window around each word (window size 2, 5, $\ldots$)
\item Collect co-occurrence counts for each pair of words in the vocabulary.
\item Create a matrix $X$ where each element $X_{i,j} = c(w_i, w_j)$
\item $c(w_i, w_j)$ is the number of times we observe word $w_i$ and $w_j$ together
\item $X$ is going to be very sparse (lots of zeroes)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Window based co-occurrence matrix}
\begin{center}
\includegraphics[scale=.45]{figures/wordvectors/docs}	
\end{center}
\end{frame}

\begin{frame}
\frametitle{Window based co-occurrence matrix}
\begin{center}
\includegraphics[scale=.4]{figures/wordvectors/wcm}	
\end{center}
\end{frame}

\begin{frame}
\frametitle{Singular Value Decomposition}
\begin{itemize}[<+->]
\item Collect $X = |V| \times |V|$ word co-occurrence matrix.
\item Apply SVD on $X$ to get $X = U S V^T$
\pause
\begin{block}{Transpose}
Transpose of $V$ is $V^T$ which switches the row and column of $V$
\end{block}
\item Select first $k$ columns of $U$ to get $k$-dimensional vectors
\item The matrix $S$ is a diagonal matrix with entries $\sigma_1, \ldots, \sigma_i, \ldots, \sigma_{|V|}$
\end{itemize}
\pause 
\begin{alertblock}{Variance}
The amount of variance captured by the first $k$ dimensions is given by
\[ \frac{\sum_{i=1}^k \sigma_i}{\sum_{i=1}^{|V|} \sigma_i} \]\end{alertblock}
\end{frame}

\begin{frame}
\frametitle{Dimensionality reduction with SVD}
\framesubtitle{Figure from \cite{cs224n}}
\centering
\includegraphics[scale=.35]{figures/wordvectors/svdapply}
\end{frame}

\begin{frame}
\frametitle{Dimensionality reduction with SVD}
\framesubtitle{Figure from \cite{cs224n}}
\centering
\includegraphics[scale=.38]{figures/wordvectors/reducedim}
\end{frame}

\begin{frame}
\frametitle{Why SVD is not the ideal solution}
\begin{itemize}[<+->]
	\item Computational complexity is high ${\cal O}(|V|^3)$
	\item Cannot be trained as part of a larger model. 
	\item It is not a component that can be part of a larger neural network
	\item Cannot be trained discriminatively for a particular task
\end{itemize}
\end{frame}

\section{Word2Vec}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Word2Vec}
\begin{itemize}[<+->]
	\item Word2Vec is a family of model + learning algorithm
	\item The goal is to learn dense word vectors
\end{itemize}
\pause
\begin{alertblock}{Continuous bag of words}
	\begin{itemize}[<+->]
		\item Takes the average of the context; predicts the target word
		\item Trained with gradient descent on cross entropy loss for word prediction 
	\end{itemize}
\end{alertblock}
\pause
\begin{alertblock}{Skip-gram}
	\begin{itemize}[<+->]
		\item Considers each context word independently and constructs (target-word, context-word) pairs
		\item Trained using negative sampling and loss on predicting good vs.\ bad pairs
	\end{itemize}
\end{alertblock}
\end{frame}

\begin{frame}
\frametitle{Word2Vec: Continuous Bag of Words}
\begin{alertblock}{CBOW}
\begin{tabbing}
the \= general \= commanded \= the \= troops\kill \\
the \> general \> \rlap{\underline{\hphantom{commanded}}} \> the \> troops
\end{tabbing}
Predicting a center word from the surrounding words \\
(also window-based)
\end{alertblock}
\pause 
\begin{block}{For each word we want to learn two vectors:}
\begin{itemize}
	\item $v_i \in \mathbb{R}^k$ (input vector) when the word $w_i$ is in the context
	\item $u_i \in \mathbb{R}^k$ (output vector) when the word $u_i$ is in the center 
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Word2Vec: Continuous Bag of Words}
\begin{block}{Algorithm}
\begin{tabbing}
the \= general \= commanded \= the \= troops\kill \\
the \> general \> \rlap{\underline{\hphantom{commanded}}} \> the \> troops \\
$v_{\textrm{the}}$ \> $v_{\textrm{general}}$ \> \> $v_{\textrm{the}}$ \> $v_{\textrm{troops}}$ 
\end{tabbing}
\begin{itemize}[<+->]
	\item Average the context vectors:
	\[ \hat{v} = \frac{v_{\textrm{the}} + v_{\textrm{general}} + v_{\textrm{the}} + v_{\textrm{troops}}}{4} \]
	\item For each word $i \in V$ we have a word vector $u_i \in \mathbb{R}^k$
	\item Compute the dot product $z_i = u_i \cdot \hat{v}$
	\item Convert $z_i \in \mathbb{R}$ into a probability:
	\[ \hat{y}_i = \frac{exp(z_i)}{\sum_{k=1}^{|V|} exp(z_k)} \]
	\item If the correct center word is $w_i$ then the max should be $\hat{y}_i$.
\end{itemize}	
\end{block}
\end{frame}

\begin{frame}
\frametitle{Word2Vec: Continuous Bag of Words}
\begin{tabbing}
the \= general \= commanded \= the \= troops\kill \\
the \> general \> \rlap{\underline{\hphantom{commanded}}} \> the \> troops \\
$v_{\textrm{the}}$ \> $v_{\textrm{general}}$ \> \> $v_{\textrm{the}}$ \> $v_{\textrm{troops}}$ 
\end{tabbing}
\begin{itemize}[<+->]
	\item Average the context vectors to get $\hat{v}$
	\item Let matrix $U = [ u_1, \ldots, u_{|V|} ] \in \mathbb{R}^{|V| \times k}$ with word vectors $u_i \in \mathbb{R}^k$
	\item Compute the matrix product $z = U \cdot \hat{v}$ where $z = [ z_1, \ldots, z_{|V|} ] \in \mathbb{R}^{|V|}$ and each $z_i \in \mathbb{R}$
	\item Compute vector $\hat{y} \in \mathbb{R}^{|V|}$. Each element $\hat{y}_i = \frac{exp(z_i)}{\sum_{k=1}^{|V|} exp(z_k)} $
	\item We write this as $\hat{y} = \textrm{softmax}(z)$
	\item If the correct center word is $w_i$ then the ideal output $y$ is a one-hot vector with index $i$ as 1 and all other elements are 0.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Word2Vec: Continuous Bag of Words}
\begin{block}{Learning}
\begin{itemize}[<+->]
	\item Goal: learn $k$-dimensional word vectors $u_i, v_i$ for each $i = 1, \ldots |V|$
	\item For each training example the correct center word $w_j$ is represented as a one-hot vector $y$ where $y_j = 1$.
	\item $\hat{y} = \textrm{softmax}(U \cdot \hat{v})$ where $\hat{v}$ is the average of the context words
	\item Loss function is the cross entropy:
	\[ H(\hat{y}, y) = - \log(\hat{y}_j) \textrm{ for $j$ where $y_j = 1$} \]
	\item If $c$ is the index of the correct word, consider case where prediction $\hat{y}_c = 0.99$ then the loss or penalty is low $H(\hat{y}, y) = - 1 \cdot \log(0.99) = 0.01$
	\item If the prediction was bad $\hat{y}_c = 0.01$ then the loss is high $H(\hat{y}, y) = - 1 \cdot \log(0.01) = 4.6$
\end{itemize}	
\end{block}
\end{frame}

\begin{frame}
\frametitle{CBOW Loss Function}
\framesubtitle{Figure from \cite{melamud16}}
\includegraphics[scale=.45]{figures/wordvectors/cbowloss}	
\end{frame}

\begin{frame}
\frametitle{Gradient descent}
\begin{block}{Objective function}
\begin{eqnarray*}
\lefteqn{\textrm{Minimize } J } \\
& = & - \log P(u_c \mid \hat{v}) \\
& = & - u_c \cdot \hat{v} + \log \sum_{j=1}^{|V|} exp(u_j \cdot \hat{v})
\end{eqnarray*}
\end{block}
\end{frame}


\begin{frame}
\frametitle{Gradient descent}
\begin{itemize}[<+->]
\item Initialize $u^{(0)}$ and $v^{(0)}$ 
\item $J(u,v) = - u_c \cdot \hat{v} + \log \sum_{j=1}^{|V|} exp(u_j \cdot \hat{v})$
\item $t \leftarrow 0$
\item Iterate to minimize loss $H(\hat{y}, y)$ on each training example:
\begin{itemize}[<+->]
\item Pick a training example at random
\item Calculate: 
\begin{eqnarray*}
	\hat{y} &=& \textrm{softmax}(U \cdot \hat{v}) \\
	\Delta_u &=& \left. \frac{d J(u,v)}{d u}  \right|_{u,v = u^{(t)}, v^{(t)}} \\
	\Delta_v &=& \left. \frac{d J(u,v)}{d v}  \right|_{u,v = u^{(t)}, v^{(t)}}
\end{eqnarray*}
\item Using a learning rate $\gamma$ find new parameter values:
\begin{eqnarray*}
	\textbf{u}^{(t+1)} &\leftarrow& \textbf{u}^{(t)} - \gamma \Delta_u \\
	\textbf{v}^{(t+1)} &\leftarrow& \textbf{v}^{(t)} - \gamma \Delta_v
\end{eqnarray*}
\end{itemize}
\end{itemize}
\end{frame}

\section{GloVe}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Relationship of PMI and Word2Vec}
\begin{itemize}[<+->]
\item Word2Vec SGNS implicitly factorizes word-context PPMI matrix
\begin{equation*}
\overrightarrow{w}\cdot\overrightarrow{c} = PMI(w,c) - \log k
\end{equation*}
\begin{equation*}
W \cdot C^T = M^{PMI} - \log k
\end{equation*}
\item SGNS factorizes M into two unconstrained matrices (vs two orthonormal and one diagonal matrix)
\item Loss is not L2 loss
\note{PMI matrix is indeed low dimensional/low rank: https://www.offconvex.org/2016/02/14/word-embeddings-2/}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{GloVe}
\begin{itemize}[<+->]
\item Explicitly encode meaning as vector offsets in an embedding space
\item Aim to minimize difference of the vectors of two words and the log of their co-occurrrence count
\begin{equation*}
\hat{J} = - \sum_{ij} f(X_{ij})  ( u_j \cdot v_i + b_i + \tilde{b}_j - \log X_{ij} )^2 
\end{equation*}
$u_j=$ \textbf{word} vector for word $j$, $v_i=$ \textbf{context} vector for word $i$.\\
$X_{ij}$ is number of times word $j$ occurs in the context of word $i$.
\item Objective is factorization of the log-count matrix (shifted by bias terms)
\end{itemize}
\end{frame}

\begin{comment}
\begin{frame}
\frametitle{GloVe}
\centering
\includegraphics[scale=0.35]{figures/wordvectors/glove1.png}
\end{frame}

\begin{frame}
\frametitle{GloVe}
\centering
\includegraphics[scale=0.35]{figures/wordvectors/glove2.png}
\end{frame}
\end{comment}

\begin{frame}
\frametitle{GloVe}
\begin{alertblock}{Co-occurrence matrix}
Let $X$ denote the \textbf{word-word co-occurrence matrix}.\\
$X_{ij}$ is number of times word $j$ occurs in the context of word $i$.\\
Let $X_i = \sum_k X_{ik}$ and $P_{ij} = P(w_j \mid w_i) = \frac{X_{ij}}{X_i}$
\end{alertblock}
\pause
\begin{alertblock}{GloVe objective}
Probability that word $j$ occurs in context of word $i$: \\
$u_j=$ \textbf{word} vector for word $j$, $v_i=$ \textbf{context} vector for word $i$.\\
\[ Q_{ij} = \frac{exp(u_j \cdot v_i)}{\sum_{w=1}^{|V|} exp(u_w \cdot v_i) } \]
\pause
Compute global cross-entropy loss:
\[ J = - \sum_{i=1}^{|V|} \sum_{j=1}^{|V|} X_{ij} \log Q_{ij} \]
\end{alertblock}
\end{frame}

\begin{frame}
\frametitle{GloVe}
\begin{alertblock}{Cross Entropy Loss}
\[ J = - \sum_{i=1}^{|V|} \sum_{j=1}^{|V|} \underbrace{X_{ij}}_{X_i P_{ij}} \log Q_{ij} \]
 \[ \textrm{$X_{ij} = X_i P_{ij}$ because: } P_{ij} = \frac{X_{ij}}{\sum_{k} X_{ik}} = \frac{X_{ij}}{X_i} \]
\pause
\[ J = - \sum_i X_i \underbrace{\sum_j P_{ij} \log Q_{ij}}_{H(P_i, Q_i)} \]
where $H$ is the cross entropy of $Q_{ij}$ which uses the parameters $u, v$ wrt the observed frequencies $P_{ij}$.
\end{alertblock}
\end{frame}


\begin{frame}
\frametitle{GloVe}
\begin{alertblock}{Simplify objective function}
 In the objective $- \sum_{ij} X_i \cdot P_{ij} \log Q_{ij}$ the distribution $Q_{ij}$ requires an expensive normalization over the entire vocabulary.\\
\pause
Simplify $J$ to $\hat{J}$ using the squared error of the logs of $\hat{P}$ and $\hat{Q}$ without normalization:
\[ \hat{J} = - \sum_{i,j=1}^{|V|} \underbrace{X_i}_{\textrm{replace with function $f(X_{ij})$}} \left( \log \underbrace{\hat{Q}_{ij}}_{exp(u_j \cdot v_i)} - \log \underbrace{\hat{P}_{ij}}_{X_{ij}} \right)^2 \]
\pause
\[ \hat{J} = - \sum_{ij} f(X_{ij})  ( u_j \cdot v_i \underbrace{+ b_i + \tilde{b}_j}_{\text{add bias terms}} - \log X_{ij} )^2 \] 
\pause
The GloVe model efficiently leverages global statistical information by training only on the nonzero elements in a word-word co-occurrence matrix.
\end{alertblock}
\end{frame}


\begin{frame}
\frametitle{GloVe}
\begin{alertblock}{Objective function}
\begin{equation*}
\hat{J} = - \sum_{ij} f(X_{ij})  ( u_j \cdot v_i + b_i + \tilde{b}_j - \log X_{ij} )^2 
\end{equation*}
\end{alertblock}
\pause
\begin{alertblock}{Weighting function}
\centering
\includegraphics[scale=0.12]{figures/wordvectors/glove-function-eq.png}
\includegraphics[scale=0.12]{figures/wordvectors/glove-function.png} \\
Rare co-occurrences are less informative than frequent ones and should be weighted less.
\note{can account for 75-95\% of entries}
\end{alertblock}
\pause
\begin{alertblock}{Final word vector}
Combination of learned \textbf{word} and \textbf{context} vectors: \\
\centering
$w_i = u_i + v_i$
\end{alertblock}
\end{frame}


\begin{frame}
	\frametitle{Extensions to word vectors}
	\begin{itemize}[<+->]
		\item Subword embeddings (FastText)
		\item Phrases and multi-word expressions
		\item Cross-lingual and modal embeddings
		\item Sense embeddings
		\item Embeddings using other types of contexts
		\item Context dependent embeddings
	\end{itemize}
\end{frame}

\section{Evaluation of Word Vectors}
\frame{\tableofcontents[currentsection]}

\begin{frame}
	\frametitle{Intrinsic Evaluation}
	\begin{itemize}[<+->]
		\item Evaluation on a specific intermediate task
		\item Fast to compute performance
		\item Helps us understand the model flaws and strengths
		\item However can fool us into thinking our model is good at extrinsic tasks
		\item {\it nokia} can be close to {\it samsung} but also to {\it finland} (Nokia is Finnish)
	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{Intrinsic Evaluation}
	\begin{block}{a : b :: c : ?}
	An intrinsic evaluation can be to identify the word vector which maximizes the cosine similarity for an analogy task:
	\[ d = \textrm{argmax}_i \frac{ (x_b - x_a + x_c) \cdot x_i }{ \| x_b - x_a + x_c \| } \]	
	we identify the vector $x_d$ which maximizes the normalized dot-product between the two word vectors (cosine similarity).	
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{Intrinsic Evaluation}
Obtain data from external source for validation e.g. geography data.
	\centering
	\includegraphics[scale=0.4]{figures/wordvectors/intrinsiceval1.png}
\end{frame}


\begin{frame}
	\frametitle{Extrinsic Evaluation}
	\begin{itemize}[<+->]
		\item Evaluation on a ``real'' task
		\item Slow to compute performance
		\item If the word vectors fail on this task it is often unclear exactly why
		\item Can experiment with various training hyperparameters or model choices to improve task performance
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Parameters}
	\begin{block}{}
	Some parameters we can consider tuning on intrinsic evaluation tasks:
	\begin{itemize}[<+->]
		\item Dimension of word vectors
		\item Corpus size
		\item Corpus source / domain / type
		\item Context window size
		\item Context symmetry
	\end{itemize}
	Can you think of any other parameters to tune in a word vector model?
	\end{block}
\end{frame}


\begin{frame}
\setbeamertemplate{bibliography item}[text]
\begin{thebibliography}{10}

\bibitem{cs224n}
\alert{Christopher Manning, Richard Socher, Francois Chaubard, Michael Fang, Guillaume Genthial, Rohit Mundra.}
\newblock {Natural Language Processing with Deep Learning: Word Vectors I: Introduction, SVD and Word2Vec}
\newblock Winter 2019.


\bibitem{melamud16}
\alert{O. Melamud and J. Goldberger and I. Dagan}
\newblock context2vec: Learning Generic Context Embedding with Bidirectional LSTM.
\newblock CoNLL 2016.
\end{thebibliography}
\end{frame}

\input{acknowledgements.tex}

\end{document}

