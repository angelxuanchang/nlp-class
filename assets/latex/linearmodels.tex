\input{beamerpreamble.tex}
\begin{document}
\input{titlepage.tex}

\lecture{Classification tasks in NLP}{}

\section{Classification tasks in NLP}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Prepositional Phrases}
\begin{itemize}[<+->]
\item noun attach: {\em I bought the shirt with pockets}
\item verb attach: {\em I washed the shirt with soap}
\item As in the case of other attachment decisions in parsing: it depends on the meaning of the entire sentence -- needs world knowledge, etc.
\item Maybe there is a simpler solution: we can attempt to solve it using heuristics or associations between words
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Ambiguity Resolution: Prepositional Phrases in English}
  \begin{itemize}[<+->]
  \item Learning Prepositional Phrase Attachment: Annotated Data
\begin{tabular}{|cccc|c|}
\hline
$v$    &     $n_1$   &      $p$ & $n_2$ &       Attachment\\
\hline
join   &   board    &   as &  director & V \\
is       & chairman  &  of  & N.V.    & N \\
using  &   crocidolite & in  & filters & V \\
bring  &   attention  & to  & problem &  V \\ 
is      &  asbestos   & in &  products & N \\
making &   paper    &   for & filters & N \\
including & three     &  with & cancer &  N \\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
\hline
\end{tabular}
  \end{itemize}

\end{frame}

\begin{frame}
\frametitle{Prepositional Phrase Attachment}
\begin{tabular}{|l|c|}  \hline
Method & Accuracy \\ \hline
Always noun attachment & 59.0 \\
Most likely for each preposition & 72.2 \\
Average Human (4 head words only) & 88.2 \\
Average Human (whole sentence) & 93.2 \\  \hline
\end{tabular}

\end{frame}

\begin{frame}
\frametitle{Back-off Smoothing}
\begin{itemize}[<+->]
\item Random variable $a$ represents attachment. 
\item $a = n_1$ or $a = v$ (two-class classification)
\item We want to compute probability of noun attachment: $p(a = n_1 \mid v, n_1, p, n_2)$. 
\item Probability of verb attachment is $1 - p(a = n_1 \mid v, n_1, p, n_2)$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Back-off Smoothing}
\begin{enumerate}
\item<1-> If $f(v,n_1,p,n_2) > 0$ and $\hat{p} \neq 0.5$
{\small
\[ \hat{p}(a_{n_1} \mid  v,n_1,p,n_2) = \frac{ f(a_{n_1},v,n_1,p,n_2) }{ f(v,n_1,p,n_2)
} \]
}
\item<2-> Else if $f(v,n_1,p) + f(v,p,n_2) + f(n_1,p,n_2) > 0$ \\
and $\hat{p} \neq 0.5$
{\small
\[ \hat{p}(a_{n_1} \mid  v,n_1,p,n_2) = \frac{ f(a_{n_1},v,n_1,p) + f(a_{n_1},v,p,n_2) +
  f(a_{n_1},n_1,p,n_2) }{ f(v,n_1,p) + f(v,p,n_2) + f(n_1,p,n_2) } \]
  }
\item<3-> Else if $f(v,p) + f(n_1,p) + f(p,n_2) > 0$
{\small
\[ \hat{p}(a_{n_1} \mid  v,n_1,p,n_2) = \frac{ f(a_{n_1},v,p) + f(a_{n_1},n_1,p) +
  f(a_{n_1},p,n_2) }{ f(v,p) + f(n_1,p) + f(p,n_2) } \]
}
\item<4-> Else if $f(p) > 0$ (try choosing attachment based on preposition alone)
{\small
\[ \hat{p}(a_{n_1} \mid  v,n_1,p,n_2) = \frac{ f(a_{n_1},p) }{ f(p) } \]
}\item<5-> Else {\small
\( \hat{p}(a_{n_1} \mid  v,n_1,p,n_2) = 1.0 \)
}
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Prepositional Phrase Attachment: Results}
  \begin{itemize}[<+->]
  \item {\bf Results (Collins and Brooks 1995)}: 84.5\% accuracy \\
  with the use of some limited word classes for dates, numbers, etc.
  \item {\bf Toutanova, Manning, and Ng, 2004}: \\
use sophisticated smoothing model for PP attachment\\
86.18\% with words \& stems; with word classes: 87.54\%
  \item {\bf Merlo, Crocker and Berthouzoz, 1997}:\\
 test on multiple PPs, generalize disambiguation of 1 PP to 2-3 PPs\\
%14 structures possible for 3PPs assuming a single verb: all 14 are attested in the Treebank\\
%same model as CB95; but generalized to dealing with upto 3PPs\\
1PP: 84.3\% \ \ 2PP: 69.6\% \ \ 3PP: 43.6\% \\
{\bf Note that this is still not the real problem faced in parsing natural language}
  \end{itemize}
\end{frame}

\lecture{Probabilistic Classifiers}{}

\section{Naive Bayes Classifier}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Naive Bayes Classifier}
\begin{itemize}[<+->]
\item \textbf{x} is the input that can be represented as $d$ independent features $f_j$, $1 \leq j \leq d$
\item $y$ is the output classification
\item $P(y \mid {\bf x}) = \frac{P(y) \cdot P({\bf x} \mid y)}{P({\bf x})}$ (Bayes Rule)
\item $P({\bf x} \mid y) = \prod_{j=1}^{d} P(f_j  \mid y)$
\item $P(y \mid {\bf x}) = P(y) \cdot  \prod_{j=1}^{d} P(f_j  \mid y)$
\end{itemize}
\end{frame}

\section{Log linear models}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Log linear model}
\begin{itemize}[<+->]
\item Let there be $m$ features, $f_k(\textbf{x}, y)$ for $k = 1, \ldots, m$
\item Define a parameter vector $\textbf{w} \in \mathbb{R}^m$
\item Each $(\textbf{x}, y)$ pair is mapped to score:
\[ s(\textbf{x}, y) = \sum_k w_k \cdot f_k(\textbf{x}, y) \]
\item Using inner product notation:
\begin{eqnarray*}
\textbf{w} \cdot \textbf{f}(\textbf{x}, y) & = & \sum_k w_k \cdot f_k(\textbf{x}, y) \\
s(\textbf{x}, y) & = & \textbf{w} \cdot \textbf{f}(\textbf{x}, y) 
\end{eqnarray*}
\item To get a probability from the score: Renormalize! 
\[ \Pr(y \mid \textbf{x}, \textbf{w}) = \frac{exp\left(s(\textbf{x}, y)\right)}{\sum_{y'} exp\left(s(\textbf{x}, y')\right)} \]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Log linear model}
\begin{itemize}[<+->]
\item The name `log-linear model' comes from:
\[ \log \Pr(y \mid \textbf{x}, \textbf{w}) = \underbrace{\textbf{w} \cdot \textbf{f}(\textbf{x}, y)}_{\textrm{linear term}} - \underbrace{\log \sum_{y'} exp\left( \textbf{w} \cdot \textbf{f}(\textbf{x}, y') \right)}_{\textrm{normalization term}} \]
\item Once the weights are learned, we can perform predictions using these features.
\item The goal: to find $\textbf{w}$ that maximizes the log likelihood $L(\textbf{w})$ of the labeled training set containing $(\textbf{x}_i, y_i)$ for $i = 1 \ldots n$
\begin{eqnarray*}
L(\textbf{w}) &=& \sum_{i} \log \Pr(y_i \mid \textbf{x}_i, \textbf{w}) \\
&=& \sum_i \textbf{w} \cdot \textbf{f}(\textbf{x}_i, y_i) - \sum_i \log \sum_{y'} exp\left( \textbf{w} \cdot \textbf{f}(\textbf{x}_i, y') \right) 
\end{eqnarray*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Log linear model}
\begin{itemize}[<+->]
\item Maximize:
\begin{eqnarray*}
L(\textbf{w}) &=& \sum_i \textbf{w} \cdot \textbf{f}(\textbf{x}_i, y_i) - \sum_i \log \sum_{y'} exp\left( \textbf{w} \cdot \textbf{f}(\textbf{x}_i, y') \right) 
\end{eqnarray*}
\item Calculate gradient:
\begin{eqnarray*}
\lefteqn{\left. \frac{d L(\textbf{w})}{d \textbf{w}} \right|_{\textbf{w}} } \\
&=& \sum_i \textbf{f}(\textbf{x}_i, y_i) - \sum_i \frac{1}{\sum_{y''} exp\left(\textbf{w} \cdot \textbf{f}(x_i, y'')\right)} \\
&& \sum_{y'} \textbf{f}(\textbf{x}_i, y')  \cdot exp\left( \textbf{w} \cdot \textbf{f}(\textbf{x}_i, y') \right) \\
&=& \sum_i \textbf{f}(\textbf{x}_i, y_i) - \sum_i \sum_{y'} \textbf{f}(\textbf{x}_i, y') \frac{exp\left( \textbf{w} \cdot \textbf{f}(\textbf{x}_i, y') \right)}{\sum_{y''} exp\left(\textbf{w} \cdot \textbf{f}(x_i, y'')\right)} \\
&=& \underbrace{\sum_i \textbf{f}(\textbf{x}_i, y_i)}_{\textrm{Observed counts}} - \underbrace{\sum_i \sum_{y'} \textbf{f}(\textbf{x}_i, y') \Pr(y' \mid \textbf{x}_i, \textbf{w})}_{\textrm{Expected counts}}
\end{eqnarray*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Log linear model}
\begin{itemize}[<+->]
\item Init: $\textbf{w}^{(0)} = \textbf{0}$
\item $t \leftarrow 0$
\item Iterate until convergence:
\begin{itemize}[<+->]
\item Calculate: $\Delta = \left. \frac{d L(\textbf{w})}{d \textbf{w}}  \right|_{\textbf{w} = \textbf{w}^{(t)}}$
\item Find $\beta^\ast = \arg\max_\beta L(\textbf{w}^{(t)} + \beta \Delta)$
\item Set $\textbf{w}^{(t+1)} \leftarrow \textbf{w}^{(t)} + \beta^\ast \Delta$
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Learning the weights: $\textbf{w}$: Generalized Iterative
  Scaling}
\begin{tabbing}
$f^\# = max_{x,y} \sum_{j} f_j(x, y)$ \\
(the maximum possible feature value; needed for scaling) \pause \\
Initialize $\textbf{w}^{(0)}$ \pause \\
For \= each iteration $t$ \\
\> \texttt{expected[j]} $\leftarrow$ 0 for j = 1 .. \# of features \pause \\
\> For \= i = 1 to $\mid \textrm{training data} \mid$ \\
\>     \> For \= each feature $f_j$ \\
\>     \>     \> \texttt{expected[j]} += $f_j(x_i, y_i) \cdot P(y_i \mid x_i, \textbf{w}^{(t)})$ \pause \\
\> For \= each feature $f_j(x,y)$ \\
\>     \> \texttt{observed[j]} = $f_j(x, y) \cdot \frac{c(x,y)}{\mid \textrm{training data} \mid}$ \pause \\ 
\> For \= each feature $f_j(x,y)$ \\
\>     \> $w_j^{(t+1)} \leftarrow w_j^{(t)} \cdot \sqrt[f^\#]{\frac{\texttt{observed[j]}}{\texttt{expected[j]}}}$ 
\end{tabbing}
\par\noindent
\small{cf. Goodman, NIPS '01}
\end{frame} 

\lecture{Linear models for Tagging}{}

\section{Tagging tasks in NLP}
\frame{\tableofcontents[currentsection]}
\newcommand{\postag}[1]{{\color{red}/#1}}
\newcommand{\nertag}[1]{{\color{blue}/#1}}

\begin{frame}
\frametitle{Tagging Tasks}
\begin{block}{Tagged Sequences}
a b e e a f h j $\Rightarrow$ a\postag{Y} b\postag{Z} e\postag{Y} e\postag{Y} a\postag{Z} f\postag{X} h\postag{Z} j\postag{Y}
\end{block}
\pause
\begin{alertblock}{Example 1: Part-of-speech tagging}
Profits\postag{N} soared\postag{V} at\postag{P} Boeing\postag{N} Co.\postag{N} ,\postag{,} easily\postag{ADV} topping\postag{V} forecasts\postag{N} on\postag{P} Wall\postag{N} Street\postag{N} ,\postag{,} as\postag{P} their\postag{POSS} CEO\postag{N} Alan\postag{N} Mulally\postag{N} announced\postag{V} first\postag{ADJ} quarter\postag{N} results\postag{N} .\postag{.}
\end{alertblock}
\pause
\begin{alertblock}{Example 2: Named Entity Recognition}
Profits\postag{O} soared\postag{O} at\postag{O} Boeing\nertag{B-CO} Co.\nertag{I-CO} ,\postag{O} easily\postag{O} topping\postag{O} forecasts\postag{O} on\postag{O} Wall\nertag{B-LOC} Street\nertag{I-LOC} ,\postag{O} as\postag{O} their\postag{O} CEO\postag{O} Alan\nertag{B-PER} Mulally\nertag{I-PER} announced\postag{O} first\postag{O} quarter\postag{O} results\postag{O} .\postag{O}
\end{alertblock}
\end{frame}


\begin{frame}
\frametitle{Notation for Tagging Tasks}
\begin{itemize}[<+->]
\item Set of possible input words: ${\cal V}$
\item Set of possible tags: ${\cal T}$
\item Word sequence: $x_{[1:n]} = [x_1, \ldots, x_n]$
\item Tag sequence: $t_{[1:n]} = [t_1, \ldots, t_n]$
\item Training data is $N$ tagged sentences, the $i^{th}$ sentence has length $n_i$:
\[ (x_{[1:n]}^{(i)}, t_{[1:n]}^{(i)}) \textrm{ for } i = 1, \ldots, n \]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Independence Assumptions for Tagging}
\begin{block}{Chain Rule}
\[ P( t_{[1:n]} \mid x_{[1:n]}) = \pause \prod_{j=1}^n P(t_j \mid t_{j-1}, \ldots, t_1, x_{[1:n]}, j) \]
\end{block}
\pause
\begin{block}{Make independence assumptions}
\[ P( t_{[1:n]} \mid x_{[1:n]}) \approx \pause \prod_{j=1}^n P(t_j \mid t_{j-1}, x_{[1:n]}, j) \]
\centering {\small $j$ is the word being tagged. }\\
\centering {\color{red}\small We model the conditional probability directly: no Bayes Rule here. }
\end{block}
\pause
\begin{block}{Questions}
\begin{itemize}
\item Split up $P(t_j \mid t_{j-1}, x_{[1:n]}, j)$ into parameters?
\item How to find $\arg\max_{t_{[1:n]}} P( t_{[1:n]} \mid x_{[1:n]})$?
\end{itemize}
\end{block}
\end{frame}

\section{Log-linear models for Tagging}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Representation: finding the right parameters}
\begin{block}{Problem: Predict ?? using context, $P(?? \mid \texttt{context})$ }
Profits\postag{N} soared\postag{V} at\postag{P} Boeing\postag{??} Co. , easily topping forecasts on Wall Street , as their CEO Alan Mulally announced first quarter results .
\end{block}
\pause
\begin{block}{Representation: history}
\begin{itemize}
\item A history is a 3-tuple: $(t_{-1}, x_{[1:n]}, i)$
\item $t_{-1}$ is the previous tag (we are assuming a bigram model)
\item $x_{[1:n]}$ are the $n$ words in the input
\item $i$ is the index of the word being tagged
\item For example, for $x_4$ = Boeing:
    \begin{itemize}
    \item $t_{-1}$ = P
    \item $x_{[1:n]}$ = (Profits, soared, ..., results, .)
    \item $i$ = 4
    \end{itemize}
\end{itemize}
\end{block}
\end{frame}

\newcommand{\featurefunction}[3]{
\[ 
f_{#1}(h,t) = \left\{
\begin{array}{ll}
1 & \textrm{if #2 and $t$ = #3 } \\
0 & \textrm{otherwise}
\end{array}
\right.
\]
}

\begin{frame}
\frametitle{Feature-vectors over history-tag pairs}
\begin{block}{Take a history, tag pair $(h,t)$}
$f_k(h, t)$ for $k = 1, \ldots, m$ are \textbf{feature functions} representing the tagging decision.
\end{block}
\pause
\begin{block}{Example: Part-of-speech tagging [Ratnaparkhi 1996]}
\centering
\featurefunction{100}{current word $x_i$ is \texttt{Boeing}}{\texttt{N}}
\featurefunction{101}{$t_{-1}$ is \texttt{P}}{\texttt{N}}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Log linear model for Tagging}
\begin{itemize}[<+->]
\item Let there be $m$ features, $f_k(\textbf{x}, \textbf{y})$ for $k = 1, \ldots, m$
\item $\textbf{x} = x_{[1:n]}$ and $\textbf{y} = t_{[1:n]}$
\item Define a parameter vector $\textbf{w} \in \mathbb{R}^m$
\item Each $(\textbf{x}, \textbf{y})$ pair is mapped to score:
\[ s(\textbf{x}, \textbf{y}) = \sum_k w_k \cdot f_k(\textbf{x}, \textbf{y}) \]
\item Using inner product notation:
\begin{eqnarray*}
\textbf{w} \cdot \textbf{f}(\textbf{x}, \textbf{y}) & = & \sum_k w_k \cdot f_k(\textbf{x}, \textbf{y}) \\
s(\textbf{x}, \textbf{y}) & = & \textbf{w} \cdot \textbf{f}(\textbf{x}, \textbf{y}) 
\end{eqnarray*}
\item To get a probability from the score: Renormalize! 
\[ \Pr(\textbf{y} \mid \textbf{x}, \textbf{w}) = \frac{exp\left(s(\textbf{x}, \textbf{y})\right)}{\sum_{\textbf{y'}} exp\left(s(\textbf{x}, \textbf{y'})\right)} \]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Feature functions for Tagging}
\begin{block}{Problem}
\begin{itemize}
\item We have defined a log-linear model using feature functions: $\textbf{f(x,y)}$ 
\item We have defined parameters using a history $h$ so feature functions are: $\textbf{f}(h, t)$
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Locally normalized log-linear taggers}
\begin{block}{Conditional Distribution over history, tag pair $(h,t)$}
\[ \log \Pr(t \mid h) = \textbf{w} \cdot \textbf{f}(h, t) - log \sum_{t'} \exp \left( \textbf{w} \cdot \textbf{f}(h, t') \right) \]
\begin{itemize}
\item $\textbf{f}(h, t)$ is a vector of feature functions
\item $\textbf{w}$ is the weight vector
\end{itemize}
\end{block}
\pause
\begin{block}{Local normalization for tagging}
\begin{itemize}
\item Word sequence: $x_{[1:n]}$ and tag sequence: $t_{[1:n]}$
\item Histories $h_i = (t_{i-1}, x_{[1:n]}, i)$ 
\end{itemize}
\[ \log \Pr( t_{[1:n]} \mid x_{[1:n]} ) = \sum_{i=1}^n \log \Pr(t_i \mid h_i) \]
\end{block}
\end{frame}


\begin{frame}
\frametitle{Globally normalized log-linear taggers}
\begin{block}{Global feature function $\mathbf{\Phi}(\textbf{x}, \textbf{y})$}
\begin{itemize}
\item Word sequence: $\textbf{x} = x_{[1:n]}$ and tag sequence: $\textbf{y} = t_{[1:n]}$
\item From \textit{local} histories $h_i = (t_{i-1}, x_{[1:n]}, i)$ to global $\Phi$ values:
\end{itemize}
\[ \Phi_k(x_{[1:n]}, t_{[1:n]}) = \sum_{i=1}^n f_k(h_i, t_i) \]
\begin{itemize}
\item $\mathbf{\Phi}(\textbf{x}, \textbf{y}) = (\Phi_1, \Phi_2, \ldots, \Phi_m)$ is a \textit{global} feature vector
\item $\textbf{w}$ is the weight vector for $\mathbf{\Phi}$
\end{itemize}
\end{block}
\pause
\begin{block}{Global normalization for tagging}
\[ \log \Pr(\textbf{y} \mid \textbf{x}, \textbf{w}) = \textbf{w} \cdot \mathbf{\Phi}(\textbf{x}, \textbf{y}) - \log \sum_{\textbf{y'}} exp\left( \textbf{w} \cdot \mathbf{\Phi}(\textbf{x}, \textbf{y'}) \right) \]
\end{block}
\end{frame}

\begin{frame}
\frametitle{Conditional Random Field}
\begin{block}{Global normalization for tagging}
\[ \log \Pr(\textbf{y} \mid \textbf{x}, \textbf{w}) = \textbf{w} \cdot \mathbf{\Phi}(\textbf{x}, \textbf{y}) - \log \sum_{\textbf{y'}} exp\left( \textbf{w} \cdot \mathbf{\Phi}(\textbf{x}, \textbf{y'}) \right) \]
\begin{itemize}
\item This model is also called a conditional random field (CRF)
\end{itemize}
\end{block}
\pause
\begin{block}{Algorithms for training and decoding}
\begin{itemize}
\item Global normalization could be expensive: requires sum over exponentially many terms $\textbf{y'}$
\item Finding $\arg\max_{\textbf{y}} \log \Pr(\textbf{y} \mid \textbf{x})$ can be accomplished using the Viterbi algorithm.
\item Training: finding the weight vector $\textbf{w}$ can be done using a variant of the Forward algorithm.
\end{itemize}
\end{block}
\end{frame}

\input{acknowledgements.tex}

\end{document}

\begin{frame}
\frametitle{Maximum Entropy}
\begin{itemize}[<+->]
\item The log-linear model has an interpretation as a {\it maximum entropy} model.
\item For observed events, maximize likelihood. For unobserved events, from all
consistent models pick the one with maximum entropy.
\item The maximum entropy principle: related to Occam's razor and
  other similar justifications for scientific inquiry
\item Make the minimum possible assumptions about unseen data
\item Also: Laplace's {\em Principle of Insufficient Reason}: when one
  has no information to distinguish between the probability of two
  events, the best strategy is to consider them equally likely
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Logistic Regression}
\begin{itemize}
\item models effects of explanatory variables on binary valued
  variable
\item observations ${\bf x} = \{x_1, \ldots, x_j\}$ with success given
  by $q({\bf x})$: \[ q({\bf x}) = \frac{e^{g({\bf x})}}{1 + e^{g({\bf
  x})}} \] and \[ g({\bf x}) = \beta_0 + \sum_{j=1}^{k} \beta_j x_j \]
\end{itemize} 
\end{frame} 

\begin{frame}
\frametitle{Logistic Regression}
\begin{itemize}
\item probability that observations lead to success, or $p(a=1 \mid
  b)$:
 \[ p(a=1 \mid b) = \frac{e^{g(b)}}{1 + e^{g(b)}} \] where
 \[ g(b) = \beta_0 f_0(1,b) + \sum_{j=1}^{k} \beta_j f_j(1,b) \]
\item $\beta_j = \textrm{log} \alpha_j$, $f_0(1,b) = 1$ and $f_j(1,b) = x_j$
\end{itemize} 
\end{frame} 
