\input{beamerpreamble.tex}
\begin{document}
\input{titlepage.tex}

\lecture{Classification tasks in NLP}{}

\section{Classification tasks in NLP}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Prepositional Phrases}
\begin{itemize}[<+->]
\item noun attach: {\em I bought the shirt with pockets}
\item verb attach: {\em I washed the shirt with soap}
\item As in the case of other attachment decisions in parsing: it depends on the meaning of the entire sentence -- needs world knowledge, etc.
\item Maybe there is a simpler solution: we can attempt to solve it using heuristics or associations between words
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Ambiguity Resolution: Prepositional Phrases in English}
  \begin{itemize}[<+->]
  \item Learning Prepositional Phrase Attachment: Annotated Data
\begin{tabular}{|cccc|c|}
\hline
$v$    &     $n_1$   &      $p$ & $n_2$ &       Attachment\\
\hline
join   &   board    &   as &  director & V \\
is       & chairman  &  of  & N.V.    & N \\
using  &   crocidolite & in  & filters & V \\
bring  &   attention  & to  & problem &  V \\ 
is      &  asbestos   & in &  products & N \\
making &   paper    &   for & filters & N \\
including & three     &  with & cancer &  N \\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
\hline
\end{tabular}
  \end{itemize}

\end{frame}

\begin{frame}
\frametitle{Prepositional Phrase Attachment}
\begin{tabular}{|l|c|}  \hline
Method & Accuracy \\ \hline
Always noun attachment & 59.0 \\
Most likely for each preposition & 72.2 \\
Average Human (4 head words only) & 88.2 \\
Average Human (whole sentence) & 93.2 \\  \hline
\end{tabular}

\end{frame}

\begin{frame}
\frametitle{Back-off Smoothing}
\begin{itemize}[<+->]
\item Random variable $a$ represents attachment. 
\item $a = n_1$ or $a = v$ (two-class classification)
\item We want to compute probability of noun attachment: $p(a = n_1 \mid v, n_1, p, n_2)$. 
\item Probability of verb attachment is $1 - p(a = n_1 \mid v, n_1, p, n_2)$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Back-off Smoothing}
\begin{enumerate}
\item<1-> If $f(v,n_1,p,n_2) > 0$ and $\hat{p} \neq 0.5$
{\small
\[ \hat{p}(a_{n_1} \mid  v,n_1,p,n_2) = \frac{ f(a_{n_1},v,n_1,p,n_2) }{ f(v,n_1,p,n_2)
} \]
}
\item<2-> Else if $f(v,n_1,p) + f(v,p,n_2) + f(n_1,p,n_2) > 0$ \\
and $\hat{p} \neq 0.5$
{\small
\[ \hat{p}(a_{n_1} \mid  v,n_1,p,n_2) = \frac{ f(a_{n_1},v,n_1,p) + f(a_{n_1},v,p,n_2) +
  f(a_{n_1},n_1,p,n_2) }{ f(v,n_1,p) + f(v,p,n_2) + f(n_1,p,n_2) } \]
  }
\item<3-> Else if $f(v,p) + f(n_1,p) + f(p,n_2) > 0$
{\small
\[ \hat{p}(a_{n_1} \mid  v,n_1,p,n_2) = \frac{ f(a_{n_1},v,p) + f(a_{n_1},n_1,p) +
  f(a_{n_1},p,n_2) }{ f(v,p) + f(n_1,p) + f(p,n_2) } \]
}
\item<4-> Else if $f(p) > 0$ (try choosing attachment based on preposition alone)
{\small
\[ \hat{p}(a_{n_1} \mid  v,n_1,p,n_2) = \frac{ f(a_{n_1},p) }{ f(p) } \]
}\item<5-> Else {\small
\( \hat{p}(a_{n_1} \mid  v,n_1,p,n_2) = 1.0 \)
}
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Prepositional Phrase Attachment: Results}
  \begin{itemize}[<+->]
  \item {\bf Results (Collins and Brooks 1995)}: 84.5\% accuracy \\
  with the use of some limited word classes for dates, numbers, etc.
  \item {\bf Toutanova, Manning, and Ng, 2004}: \\
use sophisticated smoothing model for PP attachment\\
86.18\% with words \& stems; with word classes: 87.54\%
  \item {\bf Merlo, Crocker and Berthouzoz, 1997}:\\
 test on multiple PPs, generalize disambiguation of 1 PP to 2-3 PPs\\
%14 structures possible for 3PPs assuming a single verb: all 14 are attested in the Treebank\\
%same model as CB95; but generalized to dealing with upto 3PPs\\
1PP: 84.3\% \ \ 2PP: 69.6\% \ \ 3PP: 43.6\% \\
  \end{itemize}
\end{frame}

\lecture{Probabilistic Classifiers}{}

\section{Naive Bayes Classifier}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Naive Bayes Classifier}
\begin{itemize}[<+->]
\item \textbf{x} is the input that can be represented as $d$ independent features $f_j$, $1 \leq j \leq d$
\item $y$ is the output classification
\item $P(y \mid {\bf x}) = \frac{P(y) \cdot P({\bf x} \mid y)}{P({\bf x})}$ (Bayes Rule)
\item $P({\bf x} \mid y) = \prod_{j=1}^{d} P(f_j  \mid y)$
\item $P(y \mid {\bf x}) = P(y) \cdot  \prod_{j=1}^{d} P(f_j  \mid y)$
\end{itemize}
\end{frame}

\section{Log linear models}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Log linear model}
\begin{itemize}[<+->]
\item The model classifies input into output labels $y \in {\cal Y}$
\item Let there be $m$ features, $f_k(\textbf{x}, y)$ for $k = 1, \ldots, m$
\item Define a parameter vector $\textbf{v} \in \mathbb{R}^m$
\item Each $(\textbf{x}, y)$ pair is mapped to score:
\[ s(\textbf{x}, y) = \sum_k v_k \cdot f_k(\textbf{x}, y) \]
\item Using inner product notation:
\begin{eqnarray*}
\textbf{v} \cdot \textbf{f}(\textbf{x}, y) & = & \sum_k v_k \cdot f_k(\textbf{x}, y) \\
s(\textbf{x}, y) & = & \textbf{v} \cdot \textbf{f}(\textbf{x}, y) 
\end{eqnarray*}
\item To get a probability from the score: Renormalize! 
\[ \Pr(y \mid \textbf{x}; \textbf{v}) = \frac{exp\left(s(\textbf{x}, y)\right)}{\sum_{y' \in {\cal Y}} exp\left(s(\textbf{x}, y')\right)} \]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Log linear model}
\begin{itemize}[<+->]
\item The name `log-linear model' comes from:
\[ \log \Pr(y \mid \textbf{x}; \textbf{v}) = \underbrace{\textbf{v} \cdot \textbf{f}(\textbf{x}, y)}_{\textrm{linear term}} - \underbrace{\log \sum_{y'} exp\left( \textbf{v} \cdot \textbf{f}(\textbf{x}, y') \right)}_{\textrm{normalization term}} \]
\item Once the weights $\textbf{v}$ are learned, we can perform predictions using these features.
\item The goal: to find $\textbf{v}$ that maximizes the log likelihood $L(\textbf{v})$ of the labeled training set containing $(\textbf{x}_i, y_i)$ for $i = 1 \ldots n$
\begin{eqnarray*}
L(\textbf{v}) &=& \sum_{i} \log \Pr(y_i \mid \textbf{x}_i; \textbf{v}) \\
&=& \sum_i \textbf{v} \cdot \textbf{f}(\textbf{x}_i, y_i) - \sum_i \log \sum_{y'} exp\left( \textbf{v} \cdot \textbf{f}(\textbf{x}_i, y') \right) 
\end{eqnarray*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Log linear model}
\begin{itemize}[<+->]
\item Maximize:
\begin{eqnarray*}
L(\textbf{v}) &=& \sum_i \textbf{v} \cdot \textbf{f}(\textbf{x}_i, y_i) - \sum_i \log \sum_{y'} exp\left( \textbf{v} \cdot \textbf{f}(\textbf{x}_i, y') \right) 
\end{eqnarray*}
\item Calculate gradient:
\begin{eqnarray*}
\lefteqn{\left. \frac{d L(\textbf{v})}{d \textbf{v}} \right|_{\textbf{v}} } \\
&=& \sum_i \textbf{f}(\textbf{x}_i, y_i) - \sum_i \frac{1}{\sum_{y''} exp\left(\textbf{v} \cdot \textbf{f}(x_i, y'')\right)} \\
&& \sum_{y'} \textbf{f}(\textbf{x}_i, y')  \cdot exp\left( \textbf{v} \cdot \textbf{f}(\textbf{x}_i, y') \right) \\
&=& \sum_i \textbf{f}(\textbf{x}_i, y_i) - \sum_i \sum_{y'} \textbf{f}(\textbf{x}_i, y') \frac{exp\left( \textbf{v} \cdot \textbf{f}(\textbf{x}_i, y') \right)}{\sum_{y''} exp\left(\textbf{v} \cdot \textbf{f}(x_i, y'')\right)} \\
&=& \underbrace{\sum_i \textbf{f}(\textbf{x}_i, y_i)}_{\textrm{Observed counts}} - \underbrace{\sum_i \sum_{y'} \textbf{f}(\textbf{x}_i, y') \Pr(y' \mid \textbf{x}_i; \textbf{v})}_{\textrm{Expected counts}}
\end{eqnarray*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Log linear model}
\begin{itemize}[<+->]
\item Init: $\textbf{v}^{(0)} = \textbf{0}$
\item $t \leftarrow 0$
\item Iterate until convergence:
\begin{itemize}[<+->]
\item Calculate: $\Delta = \left. \frac{d L(\textbf{v})}{d \textbf{v}}  \right|_{\textbf{v} = \textbf{v}^{(t)}}$
\item Find $\beta^\ast = \arg\max_\beta L(\textbf{v}^{(t)} + \beta \Delta)$
\item Set $\textbf{v}^{(t+1)} \leftarrow \textbf{v}^{(t)} + \beta^\ast \Delta$
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Learning the weights: $\textbf{v}$: Generalized Iterative
  Scaling}
\begin{tabbing}
$f^\# = max_{x,y} \sum_{j} f_j(x, y)$ \\
(the maximum possible feature value; needed for scaling) \pause \\
Initialize $\textbf{v}^{(0)}$ \pause \\
For \= each iteration $t$ \\
\> \texttt{expected[j]} $\leftarrow$ 0 for j = 1 .. \# of features \pause \\
\> For \= i = 1 to $\mid \textrm{training data} \mid$ \\
\>     \> For \= each feature $f_j$ \\
\>     \>     \> \texttt{expected[j]} += $f_j(x_i, y_i) \cdot P(y_i \mid x_i; \textbf{v}^{(t)})$ \pause \\
\> For \= each feature $f_j(x,y)$ \\
\>     \> \texttt{observed[j]} = $f_j(x, y) \cdot \frac{c(x,y)}{\mid \textrm{training data} \mid}$ \pause \\ 
\> For \= each feature $f_j(x,y)$ \\
\>     \> $v_j^{(t+1)} \leftarrow v_j^{(t)} \cdot \sqrt[f^\#]{\frac{\texttt{observed[j]}}{\texttt{expected[j]}}}$ 
\end{tabbing}
\par\noindent
\small{cf. Goodman, NIPS '01}
\end{frame} 

\input{acknowledgements.tex}

\end{document}

\begin{frame}
\frametitle{Maximum Entropy}
\begin{itemize}[<+->]
\item The log-linear model has an interpretation as a {\it maximum entropy} model.
\item For observed events, maximize likelihood. For unobserved events, from all
consistent models pick the one with maximum entropy.
\item The maximum entropy principle: related to Occam's razor and
  other similar justifications for scientific inquiry
\item Make the minimum possible assumptions about unseen data
\item Also: Laplace's {\em Principle of Insufficient Reason}: when one
  has no information to distinguish between the probability of two
  events, the best strategy is to consider them equally likely
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Logistic Regression}
\begin{itemize}
\item models effects of explanatory variables on binary valued
  variable
\item observations ${\bf x} = \{x_1, \ldots, x_j\}$ with success given
  by $q({\bf x})$: \[ q({\bf x}) = \frac{e^{g({\bf x})}}{1 + e^{g({\bf
  x})}} \] and \[ g({\bf x}) = \beta_0 + \sum_{j=1}^{k} \beta_j x_j \]
\end{itemize} 
\end{frame} 

\begin{frame}
\frametitle{Logistic Regression}
\begin{itemize}
\item probability that observations lead to success, or $p(a=1 \mid
  b)$:
 \[ p(a=1 \mid b) = \frac{e^{g(b)}}{1 + e^{g(b)}} \] where
 \[ g(b) = \beta_0 f_0(1,b) + \sum_{j=1}^{k} \beta_j f_j(1,b) \]
\item $\beta_j = \textrm{log} \alpha_j$, $f_0(1,b) = 1$ and $f_j(1,b) = x_j$
\end{itemize} 
\end{frame} 
