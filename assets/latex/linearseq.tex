\input{beamerpreamble.tex}
\begin{document}
\input{titlepage.tex}

\lecture{Linear models for Tagging}{}

\section{Tagging tasks in NLP}
\frame{\tableofcontents[currentsection]}
\newcommand{\postag}[1]{{\color{red}/#1}}
\newcommand{\nertag}[1]{{\color{blue}/#1}}

\begin{frame}
\frametitle{Tagging Tasks}
\begin{block}{Tagged Sequences}
a b e e a f h j $\Rightarrow$ a\postag{Y} b\postag{Z} e\postag{Y} e\postag{Y} a\postag{Z} f\postag{X} h\postag{Z} j\postag{Y}
\end{block}
\pause
\begin{alertblock}{Example 1: Part-of-speech tagging}
Profits\postag{N} soared\postag{V} at\postag{P} Boeing\postag{N} Co.\postag{N} ,\postag{,} easily\postag{ADV} topping\postag{V} forecasts\postag{N} on\postag{P} Wall\postag{N} Street\postag{N} ,\postag{,} as\postag{P} their\postag{POSS} CEO\postag{N} Alan\postag{N} Mulally\postag{N} announced\postag{V} first\postag{ADJ} quarter\postag{N} results\postag{N} .\postag{.}
\end{alertblock}
\pause
\begin{alertblock}{Example 2: Named Entity Recognition}
Profits\postag{O} soared\postag{O} at\postag{O} Boeing\nertag{B-CO} Co.\nertag{I-CO} ,\postag{O} easily\postag{O} topping\postag{O} forecasts\postag{O} on\postag{O} Wall\nertag{B-LOC} Street\nertag{I-LOC} ,\postag{O} as\postag{O} their\postag{O} CEO\postag{O} Alan\nertag{B-PER} Mulally\nertag{I-PER} announced\postag{O} first\postag{O} quarter\postag{O} results\postag{O} .\postag{O}
\end{alertblock}
\end{frame}


\begin{frame}
\frametitle{Notation for Tagging Tasks}
\begin{itemize}[<+->]
\item Set of possible input words: ${\cal V}$
\item Set of possible tags: ${\cal T}$
\item Word sequence: $x_{[1:n]} = [x_1, \ldots, x_n]$
\item Tag sequence: $t_{[1:n]} = [t_1, \ldots, t_n]$
\item Training data is $N$ tagged sentences, the $i^{th}$ sentence has length $n_i$:
\[ (x_{[1:n]}^{(i)}, t_{[1:n]}^{(i)}) \textrm{ for } i = 1, \ldots, n \]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Independence Assumptions for Tagging}
\begin{block}{Chain Rule}
\[ P( t_{[1:n]} \mid x_{[1:n]}) = \pause \prod_{j=1}^n P(t_j \mid t_{j-1}, \ldots, t_1, x_{[1:n]}, j) \]
\end{block}
\pause
\begin{block}{Make independence assumptions}
\[ P( t_{[1:n]} \mid x_{[1:n]}) \approx \pause \prod_{j=1}^n P(t_j \mid t_{j-1}, x_{[1:n]}, j) \]
\centering {\small $j$ is the word being tagged. }\\
\centering {\color{red}\small We model the conditional probability directly: no Bayes Rule here. }
\end{block}
\pause
\begin{block}{Questions}
\begin{itemize}
\item Split up $P(t_j \mid t_{j-1}, x_{[1:n]}, j)$ into parameters?
\item How to find $\arg\max_{t_{[1:n]}} P( t_{[1:n]} \mid x_{[1:n]})$?
\end{itemize}
\end{block}
\end{frame}

\section{Log-linear models for Tagging}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Representation: finding the right parameters}
\begin{block}{Problem: Predict ?? using context, $P(?? \mid \texttt{context})$ }
Profits\postag{N} soared\postag{V} at\postag{P} Boeing\postag{??} Co. , easily topping forecasts on Wall Street , as their CEO Alan Mulally announced first quarter results .
\end{block}
\pause
\begin{block}{Representation: history}
\begin{itemize}
\item A history is a 3-tuple: $(t_{-1}, x_{[1:n]}, i)$
\item $t_{-1}$ is the previous tag (we are assuming a bigram model)
\item $x_{[1:n]}$ are the $n$ words in the input
\item $i$ is the index of the word being tagged
\item For example, for $x_4$ = Boeing:
    \begin{itemize}
    \item $t_{-1}$ = P
    \item $x_{[1:n]}$ = (Profits, soared, ..., results, .)
    \item $i$ = 4
    \end{itemize}
\end{itemize}
\end{block}
\end{frame}

\newcommand{\featurefunction}[3]{
\[ 
f_{#1}(h,t) = \left\{
\begin{array}{ll}
1 & \textrm{if #2 and $t$ = #3 } \\
0 & \textrm{otherwise}
\end{array}
\right.
\]
}

\begin{frame}
\frametitle{Feature-vectors over history-tag pairs}
\begin{block}{Take a history, tag pair $(h,t)$}
$f_k(h, t)$ for $k = 1, \ldots, m$ are \textbf{feature functions} representing the tagging decision.
\end{block}
\pause
\begin{block}{Example: Part-of-speech tagging [Ratnaparkhi 1996]}
\centering
\featurefunction{100}{current word $x_i$ is \texttt{Boeing}}{\texttt{N}}
\featurefunction{101}{$t_{-1}$ is \texttt{P}}{\texttt{N}}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Log linear model for Tagging}
\begin{itemize}[<+->]
\item Let there be $m$ features, $f_k(\textbf{x}, \textbf{y})$ for $k = 1, \ldots, m$
\item $\textbf{x} = x_{[1:n]}$ and $\textbf{y} = t_{[1:n]}$
\item Define a parameter vector $\textbf{w} \in \mathbb{R}^m$
\item Each $(\textbf{x}, \textbf{y})$ pair is mapped to score:
\[ s(\textbf{x}, \textbf{y}) = \sum_k w_k \cdot f_k(\textbf{x}, \textbf{y}) \]
\item Using inner product notation:
\begin{eqnarray*}
\textbf{w} \cdot \textbf{f}(\textbf{x}, \textbf{y}) & = & \sum_k w_k \cdot f_k(\textbf{x}, \textbf{y}) \\
s(\textbf{x}, \textbf{y}) & = & \textbf{w} \cdot \textbf{f}(\textbf{x}, \textbf{y}) 
\end{eqnarray*}
\item To get a probability from the score: Renormalize! 
\[ \Pr(\textbf{y} \mid \textbf{x}, \textbf{w}) = \frac{exp\left(s(\textbf{x}, \textbf{y})\right)}{\sum_{\textbf{y'}} exp\left(s(\textbf{x}, \textbf{y'})\right)} \]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Feature functions for Tagging}
\begin{block}{Problem}
\begin{itemize}
\item We have defined a log-linear model using feature functions: $\textbf{f(x,y)}$ 
\item We have defined parameters using a history $h$ so feature functions are: $\textbf{f}(h, t)$
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Locally normalized log-linear taggers}
\begin{block}{Conditional Distribution over history, tag pair $(h,t)$}
\[ \log \Pr(t \mid h) = \textbf{w} \cdot \textbf{f}(h, t) - log \sum_{t'} \exp \left( \textbf{w} \cdot \textbf{f}(h, t') \right) \]
\begin{itemize}
\item $\textbf{f}(h, t)$ is a vector of feature functions
\item $\textbf{w}$ is the weight vector
\end{itemize}
\end{block}
\pause
\begin{block}{Local normalization for tagging}
\begin{itemize}
\item Word sequence: $x_{[1:n]}$ and tag sequence: $t_{[1:n]}$
\item Histories $h_i = (t_{i-1}, x_{[1:n]}, i)$ 
\end{itemize}
\[ \log \Pr( t_{[1:n]} \mid x_{[1:n]} ) = \sum_{i=1}^n \log \Pr(t_i \mid h_i) \]
\end{block}
\end{frame}


\begin{frame}
\frametitle{Globally normalized log-linear taggers}
\begin{block}{Global feature function $\mathbf{\Phi}(\textbf{x}, \textbf{y})$}
\begin{itemize}
\item Word sequence: $\textbf{x} = x_{[1:n]}$ and tag sequence: $\textbf{y} = t_{[1:n]}$
\item From \textit{local} histories $h_i = (t_{i-1}, x_{[1:n]}, i)$ to global $\Phi$ values:
\end{itemize}
\[ \Phi_k(x_{[1:n]}, t_{[1:n]}) = \sum_{i=1}^n f_k(h_i, t_i) \]
\begin{itemize}
\item $\mathbf{\Phi}(\textbf{x}, \textbf{y}) = (\Phi_1, \Phi_2, \ldots, \Phi_m)$ is a \textit{global} feature vector
\item $\textbf{w}$ is the weight vector for $\mathbf{\Phi}$
\end{itemize}
\end{block}
\pause
\begin{block}{Global normalization for tagging}
\[ \log \Pr(\textbf{y} \mid \textbf{x}, \textbf{w}) = \textbf{w} \cdot \mathbf{\Phi}(\textbf{x}, \textbf{y}) - \log \sum_{\textbf{y'}} exp\left( \textbf{w} \cdot \mathbf{\Phi}(\textbf{x}, \textbf{y'}) \right) \]
\end{block}
\end{frame}

\begin{frame}
\frametitle{Conditional Random Field}
\begin{block}{Global normalization for tagging}
\[ \log \Pr(\textbf{y} \mid \textbf{x}, \textbf{w}) = \textbf{w} \cdot \mathbf{\Phi}(\textbf{x}, \textbf{y}) - \log \sum_{\textbf{y'}} exp\left( \textbf{w} \cdot \mathbf{\Phi}(\textbf{x}, \textbf{y'}) \right) \]
\begin{itemize}
\item This model is also called a conditional random field (CRF)
\end{itemize}
\end{block}
\pause
\begin{block}{Algorithms for training and decoding}
\begin{itemize}
\item Global normalization could be expensive: requires sum over exponentially many terms $\textbf{y'}$
\item Finding $\arg\max_{\textbf{y}} \log \Pr(\textbf{y} \mid \textbf{x})$ can be accomplished using the Viterbi algorithm.
\item Training: finding the weight vector $\textbf{w}$ can be done using a variant of the Forward algorithm.
\end{itemize}
\end{block}
\end{frame}

\input{acknowledgements.tex}

\end{document}

\begin{frame}
\frametitle{Maximum Entropy}
\begin{itemize}[<+->]
\item The log-linear model has an interpretation as a {\it maximum entropy} model.
\item For observed events, maximize likelihood. For unobserved events, from all
consistent models pick the one with maximum entropy.
\item The maximum entropy principle: related to Occam's razor and
  other similar justifications for scientific inquiry
\item Make the minimum possible assumptions about unseen data
\item Also: Laplace's {\em Principle of Insufficient Reason}: when one
  has no information to distinguish between the probability of two
  events, the best strategy is to consider them equally likely
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Logistic Regression}
\begin{itemize}
\item models effects of explanatory variables on binary valued
  variable
\item observations ${\bf x} = \{x_1, \ldots, x_j\}$ with success given
  by $q({\bf x})$: \[ q({\bf x}) = \frac{e^{g({\bf x})}}{1 + e^{g({\bf
  x})}} \] and \[ g({\bf x}) = \beta_0 + \sum_{j=1}^{k} \beta_j x_j \]
\end{itemize} 
\end{frame} 

\begin{frame}
\frametitle{Logistic Regression}
\begin{itemize}
\item probability that observations lead to success, or $p(a=1 \mid
  b)$:
 \[ p(a=1 \mid b) = \frac{e^{g(b)}}{1 + e^{g(b)}} \] where
 \[ g(b) = \beta_0 f_0(1,b) + \sum_{j=1}^{k} \beta_j f_j(1,b) \]
\item $\beta_j = \textrm{log} \alpha_j$, $f_0(1,b) = 1$ and $f_j(1,b) = x_j$
\end{itemize} 
\end{frame} 
