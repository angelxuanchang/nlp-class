---
layout: default
img: embedding
img_link: http://en.wikipedia.org/wiki/Center_embedding
caption: "The Embedding introduces a strange form of language whose grammar can be 'self-embedded' by computers."
title: "Homework | Competitive Grammar Writing"
active_tab: homework
---

# Homework 1: Competitive Grammar Writing

<span class="text-info">Start on {{ site.hwdates[1].startdate }}</span> |
<span class="text-warning">Due on {{ site.hwdates[1].deadline }}</span>

Get started:

    git clone https://github.com/anoopsarkar/nlp-class-hw.git
    cd nlp-class-hw/cgw

Clone your repository if you havenâ€™t done it already:

    git clone git@csil-git1.cs.surrey.sfu.ca:USER/nlpclass-1187-g-GROUP.git

Then copy over the contents of the `cgw` directory above as `hw1` in your repository.

Set up the virtual environment:

    python3 -m venv venv
    source venv/bin/activate
    pip3 install -r requirements.txt

Note that if you do not change the requirements then after you have
set up the virtual environment `venv` you can simply run the following
command to get started with your development for the homework:

    source venv/bin/activate

This task involves writing or creating weighted context-free grammars
in order to parse English sentences and utterances. The vocabulary
is fixed. 

## Background

### Notation

A context-free grammar (CFG) is defined using the following building blocks:

* $$N$$, a set of non-terminal symbols (these symbols do not appear in the input)
* $$S$$, one non-terminal from $$N$$ called the start symbol. All derivations in a CFG start from $$S$$
* $$V$$, a vocabulary of words called terminal symbols. $$N$$ and $$V$$ are disjoint
* Rules of the form: $$A \rightarrow \alpha$$ where $$A \in N$$ and $$\alpha \in (N \cup V)^\ast$$.
* Weights or frequencies or probabilities can be associated with each rule in a CFG.
* A probabilistic CFG is defined as a group of conditional probabilities $$P(\alpha \mid A)$$: one for each non-terminal $$A$$

A context-free grammar that is in extended Chomsky Normal Form
(eCNF) iff the right hand side of each CFG rule is either one
non-terminal, or two non-terminals, or one terminal symbol.

This is a grammar in a formal sense. Just like we can write a
grammar for the syntax of Python, for instance. In this exercise
we will try to write a grammar for a fragment of English.

A derivation of this CFG starts with a string (called a sentential
form) containing the start symbol $$S$$ and then replaces the
non-terminals in that string recursively with the right hand side
of a rule (if there are multiple right hand sides for the same
non-terminal we pick one of them) until only terminal symbols are
left in the string. This sequence of terminal symbols is a valid
string in the (formal) language generated by the CFG.

### The Data

Initial versions of the context-free grammar files are provided to you:

* `S1.gr`: the default grammar file contains a context-free grammar in eCNF.
* `S2.gr`: the default backoff grammar file.
* `Vocab.gr`: the vocabulary file contains rules of the type `A -> a` where `A` is a non-terminal that represents the part of speech and `a` is a word (also called a terminal symbol).

Here is a fragment of `S1.gr`. Each line is a weighted context-free
grammar rule. First column is the weight, second column is the
left-hand side non-terminal of the CFG rule and the rest of the
line is the right-hand side of the CFG rule:

    1   S1   NP VP
    1   S1   NP _VP
    1   _VP  VP Punc
    20  NP   Det Nbar
    1   NP   Proper

The non-terminal `VP` is used to keep the grammar in eCNF. The
probability of a particular rule is obtained by normalizing the
weights for each left-hand side non-terminal in the grammar. For
example, for rule `NP -> Det Nbar` the conditional probability
`P(Det Nbar | NP)` is $$\frac{20}{20+1}$$.

The grammars in `S1.gr` and `S2.gr` are connected via the following rules in `S1.gr`:

    99 TOP  S1
    1  TOP  S2
    1  S2   Misc

### Other files

* `allowed_words.txt`: This file contains all the words that are allowed. You should make sure that your grammar generates sentences using exactly the words in this file. It does not specify the part of speech for each word, so you can choose to model the ambiguity of words in terms of part of speech in the `Vocab.gr` file.
* `example-sentences.txt`: This file contains example sentences that you can use as a starting point for your grammar development. Only the first two sentences of this file can be parsed using the default `S1.gr` grammar. The rest are parsed with the backoff `S2.gr` grammar. 
* `unseen.tags`: Used to deal with unknown words. You should not have to use this file during parsing, but the parser provided to you can optionally use this file in order to deal with unknown words in the input. The parser uses the weights shown in this file and provides a lexical rule with this weight for any unknown word.

### The Parser and Generator

You are given a parser that takes sentences as input and produces
parse trees and also a generator which generates a random sample
of sentences from the weighted grammar. Parsing and generating will
be useful steps in your grammar development strategy. You can learn
the various options for running the parser and generator using the
following command.

    python3 pcfg_parse_gen.py -h

#### Parsing input

The parser provided to you reads in the grammar files and a set of
input sentences. It prints out the single most probable parse tree
for each sentence (using the weights assigned to each rule in the
input context-free grammar). 

For example given the input sentence `Arthur is the king` the parser
will return the most probable derivation of the sentence which uses
the following rules (shown with their probabilities) from the grammar
files:

    99/100 TOP    -> S1
    1/2    S1     -> NP _VP
    1/21   NP     -> Proper
    1/9    Proper -> Arthur
    1      _VP    -> VP Punc
    1      VP     -> VerbT NP
    1/6    VerbT  -> is
    20/21  NP     -> Det Nbar
    1/9    Det    -> the
    10/11  Nbar   -> Noun
    1/21   Noun   -> king

There might be many other derivations for this input string but the
derivation (which is just a list of CFG rules that fit together to
derive the input string) shown above is the most probable one
returned by the Python program provided to you using an algorithm
called the CKY algorithm which returns the argmax derivation for
any input string.

The probability of the derivation is simply the product of the
probabilities of the rules used in that derivation. For this
derivation the probability is:

<p>$$ \frac{99}{100} \times \frac{1}{2} \times \frac{1}{21} \times \frac{1}{9} \times 1 \times 1 \times \frac{1}{6} \times \frac{20}{21} \times \frac{1}{9} \times \frac{10}{11} \times \frac{1}{21} $$</p> 

The derivation can be written down as a parse tree by simply linking
the non-terminals together. The following tree is simply another
(more graphical) way to represent the derivation shown above.

    (TOP (S1 (NP (Proper Arthur) ) 
             (_VP (VP (VerbT is) 
                      (NP (Det the) 
                          (Nbar (Noun king) ))) 
                  (Punc .))) )

The parser also reports the negative
cross-entropy score for the whole set of sentences. Assume the
parser gets a text of $$n$$ sentences to parse: $$s_1, s_2, \ldots,
s_n$$ and we write $$|s_i|$$ to denote the length of each sentence
$$s_i$$. The probability assigned to each sentence by the parser is
$$P(s_1), P(s_2), \ldots, P(s_n)$$. The negative cross entropy is the
average log probability score (bits per word) and is defined as
follows:

<p>$$\textrm{score}(s_1, \ldots, s_n) = \frac{ \log P(s_1) + \log P(s_2) + \ldots + \log P(s_n) }{ |s_1| + |s_2| + \ldots + |s_n| }$$</p> 

We keep the value as negative cross entropy so that higher scores
are better (a smaller negative number implies a higher or better
score).

On the command line, to parse one sentence:

    echo "Arthur is the king ." | python3 pcfg_parse_gen.py -g S1.gr S2.gr Vocab.gr -i

Which produces the following output:

    #reading grammar file: S1.gr
    #reading grammar file: S2.gr
    #reading grammar file: Vocab.gr
    (TOP (S1 (NP (Proper Arthur) ) (_VP (VP (VerbT is) (NP (Det the) (Nbar (Noun king) ))) (Punc .))) )
	#-cross entropy (bits/word): -3.78637

To parse an entire file:

    python3 pcfg_parse_gen.py -g S1.gr S2.gr Vocab.gr -i < example_sentences.txt

Which produces the following output:

    #loading grammar files: S1.gr, S2.gr, Vocab.gr
    #reading grammar file: S1.gr
    #reading grammar file: S2.gr
    #reading grammar file: Vocab.gr

    ... skipping the parse trees ...

	#-cross entropy (bits/word): -10.2762

Notice that cross entropy values for different weighted grammars
can be compared only on the same input text.

To view the output tree for a single sentence in ASCII format:

    echo "Arthur is the king ." | python3 pcfg_parse_gen.py -g S1.gr S2.gr Vocab.gr -i | python print_tree.py

To view the output tree for a single sentence graphically:

    echo "Arthur is the king ." | python3 pcfg_parse_gen.py -g S1.gr S2.gr Vocab.gr -i | python draw_tree.py

#### Generating output

In order to aid your grammar development you can also generate
sentences from the weighted grammar to test if your grammar is
producing grammatical sentences with high probability. This homework
will also rely on sampling sentences from your `S1.gr` and `Vocab.gr` 
grammar files. 

The following command samples 20 sentences from the `S1.gr,Vocab.gr`
grammar files.

    python3 pcfg_parse_gen.py -g S1.gr Vocab.gr -o -n 20

The output looks like this:

    #reading grammar file: S1.gr
    #reading grammar file: Vocab.gr
    another coconut drinks any castle .
    a sovereign near another servant carries another castle
    a weight covers each coconut .
    the swallow carries this quest .
    that castle is any sun
    each sovereign is another defeater
    any land drinks that story .
    a corner carries each corner .
    Uther Pendragon drinks this sovereign .
    any sun carries that fruit
    another corner is no weight through this weight .
    any husk by no master drinks no quest
    the defeater rides that corner
    Guinevere drinks another swallow .
    this husk by that swallow carries each corner
    the land for every castle by every swallow carries the home .
    this pound carries another master on any chalice
    every chalice rides no coconut .
    no defeater rides no quest .
    this story rides Uther Pendragon .

To generate more sentences change the value of `-n`.

The generator will not produce any words outside of the fixed
vocabulary provided in `allowed_words.txt`.

To see how the generator does error handling for bad grammars (you
should see lots of errors of sampled words not in `allowed_words.txt`):

    python3 pcfg_parse_gen.py -g S1.gr test/Vocab_missing_Sir_Uther.gr -o -n 10000 > /dev/null

## The Challenge

The main goal of this homework is to develop a weighted context-free
grammar in the files `S1.gr`, `S2.gr` and `Vocab.gr`. Your challenge
is to provide grammar files that provide the best negative cross
entropy score on the following data:

* We will sample 100 sentences from `S1.gr` and `Vocab.gr` from each of the 
  submissions after the homework deadline and concatenate these sentences 
  to form the `sampled_test` set.
* We have a pre-determined set of sentences from the same distribution that 
  produced `example-sentences.txt`. These sentences form the `test` set.

Your overall grade for this homework depends on the negative cross
entropy score on `sampled_test` and `test`. For parsing this data
we will use `pcfg_parse_gen.py` using your `S1.gr`, `S2.gr` and
`Vocab.gr` grammar files. We will use pruning and priors (see the
code for details) and a beam size of 0.0001.

Note that `S2.gr` is **not** used for sampling sentences but it
is used for parsing sentences.

**Be careful about tokenization**. Check `allowed_words.txt` and you
will see that certain contracted morphemes are separated such
as: `'ll`, `'s`, `'ve`, `n't`, etc.

### Default Submission

The following files have been provided to you and they are
the default files for this homework:

* Grammar files: `S1.gr`, `S2.gr` and `Vocab.gr` 
* Python notebook: `cgw-default.ipynb`

You should submit your grammar files and your Python
notebook `cgw.ipynb`.

### The Baseline

The baseline method for `S1.gr` is to use `example-sentences.txt`
and `devset.txt` to create a weighted context-free grammar from the
parse trees (some trees are provided in `devset.trees`). You might
want to augment this data with other sentences from the same
distribution. Also, see the section on sharing samples below.

You can improve the grammar by iteratively extracting a grammar and
improving the weighted grammar. If you can find more sentences to
parse, use an automatic parser that can produce parse trees from
the [Penn Treebank](https://catalog.ldc.upenn.edu/ldc99t42).  Note
that you do not need access to the treebank, you just need a parser
that was trained on it.

For `S2.gr` a good baseline is to exploit how often a word follows
another word in `example-sentences.txt` and `devset.txt` or sentences
sampled from other group grammars to produce the probability for
generating a sentence.

For `Vocab.gr` the baseline should at least cover all the words in
`allowed_words.txt` and you should either provide or infer the part
of speech or pre-terminal non-terminal category.

### Your Task

Provide the grammar files  `S1.gr`, `S2.gr` and `Vocab.gr` and
provide your Python notebook `cgw.ipynb` which should be the
documentation of your homework.

You have to document your development of your grammars in your
Python notebook called `cgw.ipynb` in your submission.

Clone your repository if you haven't done it already:

    git clone git@csil-git1.cs.surrey.sfu.ca:USER/nlpclass-{{ site.semcode }}-g-GROUP.git

Your repo will be cloned into a new directory called `nlpclass-{{ site.semcode }}-g-GROUP`.  
In this directory create a new `hw1` directory and put all your
source and grammar files in this directory and add, commit and push
those files to GitLab.

Before you submit your homework add a file `doc/README.username`
that documents the work done by each `username` in your group. Group
members can get zero marks if they do not have this file that shows
that they worked on the homework equally with other group members.
Put any instructions to the TA and instructor in `doc/README.txt`
or `doc/README.md`.

### Submit your homework on Coursys

When you are ready to submit go to GitLab and select `New tag` to
create a new tag. For `Tag name` use `hw1` and optionally write a
`Message`. Then select `Create tag` to create this tag.

Go to [Coursys]({{ site.coursys }}). Under the `Homework 1`
activity submit your git repository URL. It will look like
this for some `USER` in your group called `g-GROUP`:

    git@csil-git1.cs.surrey.sfu.ca:USER/nlpclass-1187-g-GROUP.git

The instructions are provided in more detail in [Homework 0](hw0.html).

That's it. You are done with Homework 1!

### Grading

Your submission will be graded using the following
grading scheme:

* We will sample 100 sentences from `S1.gr` and `Vocab.gr` from each of the 
  submissions after the homework deadline and concatenate these sentences 
  to form the `sampled_test` set.
* We have a pre-determined set of sentences from the same distribution that 
  produced `example-sentences.txt`. These sentences form the `test` set.

Your overall grade for this homework depends on the negative cross
entropy score on `sampled_test` and `test`. For parsing this data
we will use `pcfg_parse_gen.py` using your `S1.gr`, `S2.gr` and
`Vocab.gr` grammar files.

Note that `S2.gr` is **not** used for sampling sentences but it
is used for parsing sentences.

Your overall grade is a combination of:

1. Percentile score on `sampled_test` out of 10 points. We will divide up the percentile score into tenths, e.g. 90th percentile or higher gets 10, 80th percentile or higher gets 9, etc.
1. Percentile score on `test` out of 10 points. We will divide up the percentile score into tenths (same as above)
1. 10 points for your documentation of work in the Python notebook assigned by the TAs. Include what was done, the different experiments you tried, and if you combined different approaches then how you did the combination. Remember to put a `doc/README.username` for each `username` in your group.

For the percentile comparison the score from the default grammars will be included to provide a full range of scores.

## Testing your grammar before submission

If you want to test your grammar before the final submission, you
will need to check that your entropy score on `example-sentences.txt`
is improved. But that is not enough as you will also be tested on
samples from grammars of other groups in the class. You can optionally
share output samples with each other. We've set up a method to share
output samples. Sharing uses the honor system: do not use the samples
unless you provide samples of your own.

### Uploading your samples

Fork the following repository on github.com:

    https://github.com/anoopsarkar/cgw-inclass

Add your sampled sentences to the directory `sentences-fall2018` using a filename called `your-group-name.txt` and [create a pull request from your fork](https://help.github.com/articles/creating-a-pull-request-from-a-fork/).

### Using samples from other group grammars

Clone the following repository:

	https://github.com/sfu-natlang/cgw-samples

Or pull from it to get the latest samples:

    cd cgw-samples
    git pull

Go to the `sentences-fall2018` directory and concatenate all the text files in this directory:

    cat *.txt > all-samples.txt

Then parse the `all-samples.txt` file using your grammar and `pcfg_parse_gen.py` and check your cross entropy score.

## Acknowledgements

The idea for this task and most of the data files are taken from the following paper: 

> Jason Eisner and Noah A. Smith. [Competitive Grammar Writing](http://aclweb.org/anthology/W/W08/W08-0212.pdf). In Proceedings of the ACL Workshop on Issues in Teaching Computational Linguistics, pages 97-105, Columbus, OH, June 2008.

